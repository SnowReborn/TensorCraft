<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
    <style>
        div.padded {
            padding-top: 0px;
            padding-right: 100px;
            padding-bottom: 0.25in;
            padding-left: 100px;
        }
    </style>
    <title>CS 184 Final Project Milestone report</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>

<body>
    <br />
    <h1 align="middle">Tensorcraft Milestone Report(AI plays Minecraft)</h1>
    <h2 align="middle">Xuanye(Sean) He<br>
        Yanghao(Bill) Cheng<br>
        Yunfei(David) Pan
    </h2>

    <div class="padded">
<h2><a href="../index.html">Main Page</a></h2>
<h2><a href="Final Report.html">Final Report</a></h2>
<h2><a href="https://www.youtube.com/watch?v=X2WTsYIYvsQ&feature=youtu.be">Video format explaination</a></h2>
        <h2>Summary:</h2>

        <div style="text-indent:2em">
            <p>
                Right after we began the project, I started look into tensorflow and object detections. I've clean installed my operating system to be sure nothing goes wrong(what I hoped for). After a day or two I've setted up the system for tensorflow, installed cuda and Nvidia related programs, as well as python and tensorflow. I've also installed cython in case the need of communicate with C# or C++. Second, I went on a lengthy process to get a copy of minecraft, and wrote a script in python utilizing pillow and opencv to automatically collect screenshots once a hotkey is pressed. Once I collected enough in game screenshots, I began to catagorize them into different folders. Once I done that, I utilized "labelimg" a program which labels the image with boxes, and convert them into XML format for YOLO Machine learning algorithm to be used. I spent a very long time manually labeling each screenshot in all the folders, and it was a very tiring process. Once that is done, I utilized a script which automatically converts the XML file generated by labelimg into CSV files. Then after setting up trainning parameters, I started the trainning process, and it was very rough to get it to working. I had a decent GPU, but it seems to run out memory really quickly even it had 8GB of VRAM. After a lot more debugging, I finally got the trainning to start. After many hours later, I realized my step size is too large, so i reduced from 200k to 1k, and it works some what ok. I continued to train with increased step size to 20k, after it's done, it's working as intented, with some minotr bugs.
            </p>
        </div>

        <h2>Preliminary results:</h2>
        <div style="text-indent:2em">
            
            <p>
                In order to ensure the least noise possible, I've kept minecraft running in default window, lowest graphic options possible, with FOV of 82. and lowest block visibility, gamma of 10, enabled cheats in order to disable day night cycle(ensuring always day time), and we are running in creative mode(to save tiem and ability to fly). After debugging and fine tuning the algorithm and trainning sets, we've came to our minimal expected result. We are able identify minecraft in game objects (mostly terrains at this stage). For instance, grass, trees, snow, stone, water... etc. This is useful and specifically prepared for our 2D map generation. However, there are problems. First, the FPS(frame per second) of our tensorflow program is horrible. at best it is around 12 FPS, and worst is around 3~4 FPS. This could be optimized further to achieve ideal results. Another problem is, now we have the information and data about where the object is on the screen, how do we really translate the information given by the machine learning algorithm as input to generate a 2D map? There are many edge cases to be thought out, and we are at the stage to implement 2D map generation logics, but it seems to be challenging. Third, after 20k trainning steps, the loss is still consistently jump beteewen 0.1 to 0.5, which is much higher than my test sample(cats and dogs), this could be caused by inefficient trainning step or unoptimized model usage. 
            </p>
            <div align="center">
            <tr>
            <td align="middle">
            <img src="../img/tensorboard.png" width="540px"/>
               <figcaption align="middle">
                Tensorboard which reflects real time loss and total loss in static graph. </figcaption>
            <img src="../img/minecraft1.png" width="540px"/>
            <figcaption align="middle">
        The Machine learning algorithm correctly identifies the water and the sand terrain </figcaption>
      </td>
  </tr>
            </div>  
        </div>

        <h2>Reflect on progress:</h2>
        <div style="text-indent:2em">
            <p>
                At this stage right now, we've completed our core function of machine learning detection for the minecraft. We are able to generate useful information in real time during a game play or even related media(game play footage or screenshots). However, we had two potential path for us to go in our advised plan. First is to generate 2D map from the information given. This plan is challenging after my team been trying to implement it's logic. The reason is that, there are a lot of inconsistency factors in the game. For instance. The height of player determines the vertical distance of the ground, which could be smaller or larger. Unless we utilize cheat engine or intrusive method to grab minecraft vertical distance, we are unable to judge how far are we in the sky. Let's assume we are at ground all times, this also creates problem because the angle of the camera(player's head and vision) is looking, it can drastically change the size and depth as well. So we had hard time trying to figure out how we can translate what's on the screen and compress 3D information into 2D. The second path for us is to reshade, this is much simpler task in our opinion compared to 2D map generation, because what we intented to do is to reshade the model on top of it's original renderings based on it's on screen position; and what the ML gives us is exactly that. Therefore, we are considering the possibility of abandoning the 2D map feature unless we can find a breakthough on our logic, but before that, we will be aiming for reshading enemy models depends on enemy position.(this requires me to retrain the neuronet since right now it's trained to look for terrains for our 2D map generation feature).
            </p>
        </div>
        


</body>

</html>