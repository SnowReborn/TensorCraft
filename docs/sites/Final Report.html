<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
    <style>
        div.padded {
            padding-top: 0px;
            padding-right: 100px;
            padding-bottom: 0.25in;
            padding-left: 100px;
        }
    </style>
    <title>CS 184 Final Project Milestone report</title>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>

<body>
    <br />
    <h1 align="middle">Tensorcraft Final Report(AI plays Minecraft)</h1>
    <h2 align="middle">Xuanye(Sean) He<br>
        Yanghao(Bill) Cheng<br>
        Yunfei(David) Pan
    </h2>

    <div class="padded">


<h2><a href="../index.html">Main Page</a></h2>
<h2><a href="Milestone Report.html">Milestone Report</a></h2>
<h2><a href="https://www.youtube.com/watch?v=Rh7liQbjh3E&feature=youtu.be">Final Project Video</a></h2>

<div>
    <h2>Abstract:</h2>
    <p>
        Tensorcraft consistents of 5 different programming langauges, including C#, C++, Python, X86 Assembly, 
        and Lua to achieve 3D world to 2D world compression; to put it short, we built an interactivable feature 
        rich 2D minimap for minecraft which automatically recognize and generate surrounding terrains. The 2D map 
        has various features on it's own. <br><br>

        It allows player to place customizable market with differnet texture to choose from at any place, 
        add descriptive text on that marker, zoom in and out of camera by scrolling, 
        pan and move the camera around with map mode, toggle automatic terrain generation with hotkeys, and 
        best of all, minimap player is 100% synchronized with the minecraft player in real time. <br><br>

        We utilized 
        <li>YOLO ML Algorithm to identify minecraft terrain objects, utilized cheat engine and ollydbg debuggers to 
        modify assembly code in order to obtain base address and pointers in minecraft, </li>
        <li>built C++ dynamic link library with injector for grabbing in game memory,  </li>
        <li>made TCP sockets and input output system for interprocess communication between all 5 different languages,  </li>
        <li>then lastly, we built a 2D minimap inside Unity with synchronized data directly feeding from our live game,  </li>
        <li>and all actions performed in minecraft will be translated in real time to Unity 2D minimap. </li>
    </p>
</div>

<div>
    <h2>Technical approach:</h2>
    <h3>Component 1 (Machine Learning Tensorflow): </h3>
    <p>
        First I built capture with python, utilizing win32gui to 
        achieve fast real time capture, then I made the capture follow my cursor position, adding special 
        color filter for easier ML processing. Adding hotkeys by importing Virtual Keyboard to capture 
        screenshots from minecraft and automatically catagorize into different folders. <br><br>

        I then proceed to proecss the images by using labelimg to manually label each image for the object we are trying to 
        detect, which it will generate an XML file containning the classifer number as well as the x,y 
        coordinate, and convert the XML into CSV for processing. I set the object detection to be 8 items, 
        including grass, tree, water, sand bammboo, stone, snow, and lava. <br><br>

        Next we configured all the labelmaps, model configurations, with step of 200k steps, and fired up our GPU and trained the 
        model for about 50~60 hours. The loss at the end is still hovering around 0.1~0.3, because I 
        captured insufficient data for lava block, and overtrained for grass which resulted in biased 
        trainning model for other tiles, which later we ran into issue detecting lava correctly with very 
        high false negative rate, and we don't need grass tiles after all. After the model was done building, 
        I generated a frozen inference graph to freeze the current model. <br><br>
        
        After successful creation of the inference graph, the object detection model work decently well, but with average 8 FPS(frame per second). 
        I've formatted the real time data generated by the model into string 
        TerrainMLdata = "Classifier_number , x0, x1, y0, y1". The formatted data will represent which terrain 
        object it sees, and the location it sees on the player screen. However, the generated coordinate data 
        is not pure location data for current screen resolution, but instead a "proportion" of the capture window. 
        For instance, if an a tree was detected on the upper left corner, and it takes 1/4th the game view, 
        the game view window is set to 500x500 pixels. This will translate to data of 
        "2(tree) , 0, 0.5(half width), 0, 0.5(half height)". With useful information being generated, <br><br>

        we now able utilize machine learning's data to visualize where each terrain objects are. 
        However, in order to process this data and feed it into uniy, I had to implemented TCP socket and 
        input output file system to export the data generated by running the model, and we ran into 
        hiccups where the input output system would pend a lock while read or write, so read and write 
        cannot be done at the same time, and this issue was later resolved using TCP socket.<br>
    </p>
    <h3>Component 2 (Dynamic Link Library Memory Injector / Lua x86 Assembly Debugging): </h3>
    <p>
        In this part, the main objective is to grab minecraft's useful information in real time. The data we are looking 
        for is player's coordinate in the world(x,y,z coordinates). The head tilt and rotation degree, 
        which will be denoted as Yaw and Pitch. <br><br>
        
        First I will have to find the base address for each variables, 
        as well as their data structure, and pointer offsets. I utilized Lua script to automate this process. 
        I also attached a hook for ollydbg debugger on to minecraft, and we eventually found the assembly code 
        "movss [rdi],xmm8". So we know from the debugger [rdi] translates to the address "0x2E67FD80", and we 
        found the base addrses. Lua script helped us to calculate the offset, and we eventually found the offset 
        to be like structure on the right: "OpenAL.dll"+000FEC38 -> 0x2E67FBF0, adding offset of 0x08 -> 0x2E67FD80, 
        again adding offset 0x0 eventually yield our player coordinate x pointer, which being 2E67FD80, 
        which modifies the base address of player coordinates. <br><br>
        
        Luckily, this op code controls all 3 variables,
        x, y ,z for the player. With all 3 variables, we when to memory structure view, and was able to find 
        the Pitch and Yaw structure not far from the current memory. However, because the Yaw was calcualted 
        based on how much you are facing each direction(north, easy, south, west) as proportion, we have to 
        calculate using all 4 values for a final value in degree, and at the end we will have a value which 
        keep track of Yaw in -360 degree and 360 degree. The Pitch is much easier, it has an initial value 
        of -1 and 1, so it goes from -90 degree to 90, when looking all the way down and all the way up 
        respectively. Now what I have, is 5 different variables stored in the memory. <br><br>

        Now we have player x, y, z, Pitch, Yaw. However, we can't really export this data inside a debugger, 
        we can only view, and modify. So now I've made my own Memory debugger using C++ as dynamic link library(DLL). 
        After that, I again used C++ to make memory injector to inject the DLL debugger I just made into the 
        minecraft executable file. I inputted the base address and pointers we found inside the debugger, as well 
        adding support of modifying assembly code to ensure it always return the base address. <br><br>
        
        After that, we can externally track minecraft's player 3D coordinates and Pitch, Yaw in real time. With the valuable data in 
        hands, we can finally export the data since now we have the ability to export in the external program we wrote. 
        I exported the data in the format of player _ (x, y, z, Yaw, Pitch). Lastly, again we have to feed this data 
        into unity for it to be leveraged. Instead of TCP, C++ doesn't really put a lock on the input output 
        streamreader / streamwriter. Therefore we can simply output the data and directly feed it into Unity as 
        the format mentioned above.<br>
    </p>    
    <h3>Component 3 (Unity C# logic and scripts, core functions): </h3>
    <p>
        <strong> This part of project is the main focus. </strong> <br><br>

        First I built a game world in Unity using default tiles as grass. Then I imported all the textures 
        as png files for transparency support. I angled the camera, and locked it on to my main focus 
        "SteveHead" with x, y coordinates, now I have a semi-working 2D world with Steve. Next I started 
        adding features. <br><br>
        
        First feature I added is data process and syncrhonzation of the real game to 
        allow the Steve move around in the 2D world. I utilized both my input output system and TCP socket 
        to recieve the data from python and C++, after correctly formatting and parsing them into variables, 
        my Steve head moves alongside the real Steve inside minecraft. The head Pitch and Yaw is also 
        correctly tracked after some adjustment(the values were inverted.). <br><br>
        
        Next, I figured that in order to 
        implement automatic terrain tiles placement, I have to be able to generate markers first. So why not 
        build the marker and then expand on top of that. So naturally I started building the marker features. 
        It took me a while to fully understand how to use Unity, eventually i built the canvas and text 
        inputfiled for the marker, so descriptive message can be added once marker is placed down. I added 
        additional features to support iterate through the texture list to choose ideal texture for the 
        marker for better customizability. <br><br>
        
        Next, I added zoom in and out support, and camera zoom level / 
        coordinate reset with hotkey. After I gained the ability to zoom in and out, I realize the marker 
        became too small to see and read, so I added support to scale as I zoom, so the icon will stay 
        visible with good readiability. <br><br>
        
        Now with Marker and character movement synchronzization done, 
        I realized I lack of map control since my camera is always locked on the Stevehead. I then 
        increased the degree of freedom by implementing mapmode, which added hotkey "M" support to be 
        toggled on and off. Once map mode is enabled, user will have the ability to move up down left right 
        with "wsad" keys; as well as reset the camera back to the Stevehead. <br><br>
        
        With so many hotkeys Implemented, 
        I ran into another issue, which is while typing descriptive message for the custom marker, hotkey 
        will be triggered unintentionally, therefore to combat that bug, I had to write additional functions 
        to check whether my input field is being focused at the moment. <br><br>
        
        Finally, after all the foundations 
        being laid, the core function of our project: automatic terrain tiles placement system.<br><br>
        
        After going over many iterations of designs and redesigns, I finally had some idea on how to approximate the 
        depth better. I built an square box representing the player view inside minecraft. The square is 
        then projected on onto the 2D minimap. The two main factor which determines the projection of the 
        square is player z coordinate in minecraft(height), and player Pitch in minecraft (degree of facing, up or down). <br><br>
        
        First, I did an calculation inside minecraft by enabling fly mode, and counting blocks as well as FOV(Field of View). 
        I calculated the delta of FOV changed by the height increases 2 times per 5 z coordinates. <br><br>
        
        However, the Z axis cannot be simply translated into scaler, because in 3D minecraft, the terrain 
        also has high delta, meaning terrain height varies through out the map. So for instance, if I stand 
        on 60 meter of ground, but i moved to a high ground of 80 meters, doesn't mean my FOV suddenly 
        increased 4 fold. <br><br>
        
        Another problem is that, when elevation decreases, the FOV will shrink to non 
        existance. To combat problems mentioned above, I first established a base line z value. When Unity 
        starts running, it will first measure my intial z value in minecraft, this is usually at the ground 
        im standing at. Then, any delta to the z will be computed into scaler. For instnace. I spawn in with 
        heght of 60 meter, then original z = 60. I flyed a little higher, at 65 meters, 
        then the delta z is 65-60 = 5, 5/5 + 1 = 2! Now my FOV becomes 2 times as before, and i increase 
        the scaler of my square box by 2 folds. Next, decreasing elevation below original z value will be 
        resetting the original z value to avoid shrinking and negative z value delta. <br><br>
        
        Now with height taken care of, we move on to the second factor which changes our square box - the Pitch. Pitch 
        plays very important role in estimating the depth because when you stare down with height delta = 0, 
        meaning you are directly looking down on the ground,there isn't much depth to it, and the camera 
        FOV is just a square under the player / stevehead, however, as we raising our head and Pitch degree 
        increases, we slowly extends our FOV forward, start to see things further away, and furthest 
        things will appear at the middle of the screen if the Pitch is at 0 degree. 
        (the Pitch goes from -90 to +90, -90 is looking down, 0 is looking forward, 90 is looking all the way up).<br><br>
        
        Now we know we can semi generate depth using the delta of Pitch, I started implementing the function 
        to mutate the FOV square box. Now it will be denoted as "FOV box" since the Pitch mutates it's shade 
        by extending it's "y, vertical" axies forward. The FOV box's vertical scale will be modified in 
        conjunction of both Height (z) and Pitch. The exact formula is listed below as code: <br><br>                
        
        <code>
            if (original_z == 0){<br>
                &nbsp;&nbsp;original_z = CameraMove.player_z;<br>
            }<br>
            float scale = ((CameraMove.player_z - ori_z) / 5)+1;<br>
            if (RotateByFile.Pitch < 0)<br>
            {<br>
                &nbsp;&nbsp;heightScale = (((90.01f + RotateByFile.Pitch) / 45)/2)+1;<br>
            }<br>
            transform.localScale = new Vector3(scale, scale*heightScale, 1);<br>
            square_vec = new Vector3(0, -4.5f * scale * heightScale,0);<br>
            square_vec =  Quaternion.AngleAxis(-RotateByFile.Yaw, Vector3.forward) * square_vec; <br>
            square_vec += new Vector3(MoveHead.player_x ,-MoveHead.player_y,0);<br>
            <br>
        </code>
        
        Now the mutate square function is finally complete, we can then proceed to TerrainMLdata processing. 
        We got our data from TCP socket fed from python YOLO(you only look once) machine learning algorithm. <br><br>

        The Algorithm provides data in following structure: <br>
        (Classifier identifer number , x1 proportion , x2 proportion, y1 porprotion, y2 proportion.) <br><br>

        The identifier number is just a number to differentiate each object(grass, tree , etc). 
        Then the x,y coordinates are the box locaters as where the machine learning would draw the box around the object. 
        Since it only requires 2 sets of points to draw a box, from upper left, to bottom right. 
        So the data we have will be represented in this way. Vector2D upperleft = (x1, y1) Vector2D buttomright = (x2,y2). <br><br>

        However, due to the game window size can be dynamicly changing, therefore the resolution is not constant. 
        Instead we will use proportion for the x,y coordinates. For instance, I have an tree object which takes up one 
        quarter of the screen size, and it is upperleft corner. It uses 1/4 th of the area. So the data will be 
        presented as (tree, 0, 0.5, 0, 0.5). <br><br>
        
        With this format, and the previous implementaion of our FOV box, 
        we can semi determine depth based on everything we have! Now with the machine learning automatically spit out data, 
        the function is working as intented and generating terrain texture tiles on the map automatically!  <br><br>
        
        For debug and quality of life reasons, I added hotkey "G" to toggle the feature on and off. 
        On top of that, I implmented duplication avoidance feature, if the data recieved is same as last data recieved, 
        the no tile will be placed. Also to not completely crash Unity, We only place 1 tile per second regardless 
        how many objects we detect in our ML model. After all these, we finally reached somewhat satsifactory results. 
        <br>
    </p>
    <h2>Conclusion</h2>
    <p>
        We ran into uncountable amount of bugs, and found ways to tackle them one by one. I learned a lot from this project, 
        I learned Unity from scratch, and further Consolidated my skill in game hacking; and built machine learning algorithm 
        on top of that. It was unspeakable amount of things i've learned. I feel very excited and enjoyed every second of this project.
        <br>
    </p>

</div>
<div align="center">
    <h2 align="middle">Result: More result and Demo on the video!</h2>
    <tr>
        <td align="middle">
            <img src="../img/result.png" width="1080px"/>
            <figcaption align="middle">
                2D map generated sorrounding terrains with customized Marker and useful descriptions under it! 
            </figcaption>
        </td>
    </tr>
    
</div>

<div style="text-indent:2em">
    <p>
        <h2>Team Contribution:</h2>

        Everyone contributed equally<br>
        Xuanye(Sean) He: Leading and making sure Everything is on track, guide teammembers while they are lost. Implemented memory injector, memory grabber, Machine learning, logic, Unity functions, any everything else. <br>
        Yanghao(Bill) Chen: Unity, logic design, some math, report format.<br>
        Yunfei(David) Pan: Unity, brainstorm, report format.<br>
    </p>
</div>

       

</body>

</html>
